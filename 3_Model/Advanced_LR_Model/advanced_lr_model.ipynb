{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f4ac54",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23439559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully!\n",
      "\n",
      "üöÄ Advanced Linear Regression Model\n",
      "   Goal: Improve on baseline (Kaggle: 0.92918)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")\n",
    "print(\"\\nüöÄ Advanced Linear Regression Model\")\n",
    "print(\"   Goal: Improve on baseline (Kaggle: 0.92918)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590cd65",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Pre-Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f79f6766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "\n",
      "üìä TRAINING SET:\n",
      "   Shape: 7,493 rows √ó 50 columns\n",
      "   Date range: 2013-07-01 to 2017-07-31\n",
      "\n",
      "üìä VALIDATION SET:\n",
      "   Shape: 1,841 rows √ó 50 columns\n",
      "   Date range: 2017-08-01 to 2018-07-31\n",
      "\n",
      "üìä TEST SET:\n",
      "   Shape: 1,830 rows √ó 50 columns\n",
      "   Date range: 2018-08-01 to 2019-07-30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>Warengruppe</th>\n",
       "      <th>umsatz</th>\n",
       "      <th>KielerWoche</th>\n",
       "      <th>Bewoelkung</th>\n",
       "      <th>Temperatur</th>\n",
       "      <th>Windgeschwindigkeit</th>\n",
       "      <th>Wettercode</th>\n",
       "      <th>Is_Holiday</th>\n",
       "      <th>Day_Before_Holiday</th>\n",
       "      <th>Day_After_Holiday</th>\n",
       "      <th>Is_Vacation</th>\n",
       "      <th>Vacation_Type</th>\n",
       "      <th>day_of_the_week</th>\n",
       "      <th>month</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>days_to_weekend</th>\n",
       "      <th>bewoelkung_category</th>\n",
       "      <th>wettercode_category</th>\n",
       "      <th>season</th>\n",
       "      <th>temperature_category</th>\n",
       "      <th>temp_change_1d</th>\n",
       "      <th>temp_trend_3d</th>\n",
       "      <th>umsatz_ma7</th>\n",
       "      <th>umsatz_ma14</th>\n",
       "      <th>umsatz_volatility_7d</th>\n",
       "      <th>umsatz_sum_7d</th>\n",
       "      <th>umsatz_lag1</th>\n",
       "      <th>umsatz_lag7</th>\n",
       "      <th>umsatz_pct_change</th>\n",
       "      <th>umsatz_pct_change_lag1</th>\n",
       "      <th>season_numeric</th>\n",
       "      <th>is_weekend_int</th>\n",
       "      <th>weekend_season_interaction</th>\n",
       "      <th>weekend_season_category</th>\n",
       "      <th>holiday_temp_interaction</th>\n",
       "      <th>holiday_temp_category</th>\n",
       "      <th>day_before_holiday_int</th>\n",
       "      <th>day_before_holiday_weekend_interaction</th>\n",
       "      <th>day_before_holiday_weekend_category</th>\n",
       "      <th>KielerWoche_binary</th>\n",
       "      <th>kielerweek_temp_interaction</th>\n",
       "      <th>kielerweek_temp_category</th>\n",
       "      <th>temp_season_interaction</th>\n",
       "      <th>temp_season_category</th>\n",
       "      <th>weekend_temp_interaction</th>\n",
       "      <th>weekend_temp_category</th>\n",
       "      <th>bewoelkung_weekend_interaction</th>\n",
       "      <th>bewoelkung_weekend_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1307011.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.828353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very cloudy</td>\n",
       "      <td>Drizzle (not freezing) or snow grains</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148.828353</td>\n",
       "      <td>148.828353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148.828353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_Holiday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Regular_Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_KielerWoche</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Medium_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Very cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1307012.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>535.856285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very cloudy</td>\n",
       "      <td>Drizzle (not freezing) or snow grains</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342.342319</td>\n",
       "      <td>342.342319</td>\n",
       "      <td>273.670075</td>\n",
       "      <td>684.684638</td>\n",
       "      <td>148.828353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.600499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_Holiday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Regular_Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_KielerWoche</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Medium_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Very cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1307013.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>201.198426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very cloudy</td>\n",
       "      <td>Drizzle (not freezing) or snow grains</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>295.294355</td>\n",
       "      <td>295.294355</td>\n",
       "      <td>209.971874</td>\n",
       "      <td>885.883064</td>\n",
       "      <td>535.856285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.624529</td>\n",
       "      <td>2.600499</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_Holiday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Regular_Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_KielerWoche</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Medium_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Very cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1307014.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65.890169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very cloudy</td>\n",
       "      <td>Drizzle (not freezing) or snow grains</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>237.943308</td>\n",
       "      <td>237.943308</td>\n",
       "      <td>206.273351</td>\n",
       "      <td>951.773232</td>\n",
       "      <td>201.198426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.672512</td>\n",
       "      <td>-0.624529</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_Holiday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Regular_Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_KielerWoche</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Medium_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Very cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1307015.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>317.475875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very cloudy</td>\n",
       "      <td>Drizzle (not freezing) or snow grains</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.849821</td>\n",
       "      <td>253.849821</td>\n",
       "      <td>182.144468</td>\n",
       "      <td>1269.249107</td>\n",
       "      <td>65.890169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.818259</td>\n",
       "      <td>-0.672512</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_Holiday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Regular_Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No_KielerWoche</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Medium_Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Weekday_Very cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       date  Warengruppe      umsatz  KielerWoche  Bewoelkung  \\\n",
       "0  1307011.0 2013-07-01          1.0  148.828353          NaN         6.0   \n",
       "1  1307012.0 2013-07-01          2.0  535.856285          NaN         6.0   \n",
       "2  1307013.0 2013-07-01          3.0  201.198426          NaN         6.0   \n",
       "3  1307014.0 2013-07-01          4.0   65.890169          NaN         6.0   \n",
       "4  1307015.0 2013-07-01          5.0  317.475875          NaN         6.0   \n",
       "\n",
       "   Temperatur  Windgeschwindigkeit  Wettercode  Is_Holiday  \\\n",
       "0     17.8375                 15.0        20.0         0.0   \n",
       "1     17.8375                 15.0        20.0         0.0   \n",
       "2     17.8375                 15.0        20.0         0.0   \n",
       "3     17.8375                 15.0        20.0         0.0   \n",
       "4     17.8375                 15.0        20.0         0.0   \n",
       "\n",
       "   Day_Before_Holiday  Day_After_Holiday  Is_Vacation Vacation_Type  \\\n",
       "0                 0.0                0.0          1.0        Summer   \n",
       "1                 0.0                0.0          1.0        Summer   \n",
       "2                 0.0                0.0          1.0        Summer   \n",
       "3                 0.0                0.0          1.0        Summer   \n",
       "4                 0.0                0.0          1.0        Summer   \n",
       "\n",
       "   day_of_the_week  month  is_weekend  days_to_weekend bewoelkung_category  \\\n",
       "0              0.0    7.0       False              5.0         Very cloudy   \n",
       "1              0.0    7.0       False              5.0         Very cloudy   \n",
       "2              0.0    7.0       False              5.0         Very cloudy   \n",
       "3              0.0    7.0       False              5.0         Very cloudy   \n",
       "4              0.0    7.0       False              5.0         Very cloudy   \n",
       "\n",
       "                     wettercode_category  season temperature_category  \\\n",
       "0  Drizzle (not freezing) or snow grains  Summer               Medium   \n",
       "1  Drizzle (not freezing) or snow grains  Summer               Medium   \n",
       "2  Drizzle (not freezing) or snow grains  Summer               Medium   \n",
       "3  Drizzle (not freezing) or snow grains  Summer               Medium   \n",
       "4  Drizzle (not freezing) or snow grains  Summer               Medium   \n",
       "\n",
       "   temp_change_1d  temp_trend_3d  umsatz_ma7  umsatz_ma14  \\\n",
       "0             NaN            NaN  148.828353   148.828353   \n",
       "1             0.0            NaN  342.342319   342.342319   \n",
       "2             0.0            NaN  295.294355   295.294355   \n",
       "3             0.0            0.0  237.943308   237.943308   \n",
       "4             0.0            0.0  253.849821   253.849821   \n",
       "\n",
       "   umsatz_volatility_7d  umsatz_sum_7d  umsatz_lag1  umsatz_lag7  \\\n",
       "0                   NaN     148.828353          NaN          NaN   \n",
       "1            273.670075     684.684638   148.828353          NaN   \n",
       "2            209.971874     885.883064   535.856285          NaN   \n",
       "3            206.273351     951.773232   201.198426          NaN   \n",
       "4            182.144468    1269.249107    65.890169          NaN   \n",
       "\n",
       "   umsatz_pct_change  umsatz_pct_change_lag1  season_numeric  is_weekend_int  \\\n",
       "0                NaN                     NaN             3.0             0.0   \n",
       "1           2.600499                     NaN             3.0             0.0   \n",
       "2          -0.624529                2.600499             3.0             0.0   \n",
       "3          -0.672512               -0.624529             3.0             0.0   \n",
       "4           3.818259               -0.672512             3.0             0.0   \n",
       "\n",
       "   weekend_season_interaction weekend_season_category  \\\n",
       "0                         0.0          Weekday_Summer   \n",
       "1                         0.0          Weekday_Summer   \n",
       "2                         0.0          Weekday_Summer   \n",
       "3                         0.0          Weekday_Summer   \n",
       "4                         0.0          Weekday_Summer   \n",
       "\n",
       "   holiday_temp_interaction holiday_temp_category  day_before_holiday_int  \\\n",
       "0                       0.0            No_Holiday                     0.0   \n",
       "1                       0.0            No_Holiday                     0.0   \n",
       "2                       0.0            No_Holiday                     0.0   \n",
       "3                       0.0            No_Holiday                     0.0   \n",
       "4                       0.0            No_Holiday                     0.0   \n",
       "\n",
       "   day_before_holiday_weekend_interaction day_before_holiday_weekend_category  \\\n",
       "0                                     0.0                         Regular_Day   \n",
       "1                                     0.0                         Regular_Day   \n",
       "2                                     0.0                         Regular_Day   \n",
       "3                                     0.0                         Regular_Day   \n",
       "4                                     0.0                         Regular_Day   \n",
       "\n",
       "   KielerWoche_binary  kielerweek_temp_interaction kielerweek_temp_category  \\\n",
       "0                 0.0                          0.0           No_KielerWoche   \n",
       "1                 0.0                          0.0           No_KielerWoche   \n",
       "2                 0.0                          0.0           No_KielerWoche   \n",
       "3                 0.0                          0.0           No_KielerWoche   \n",
       "4                 0.0                          0.0           No_KielerWoche   \n",
       "\n",
       "   temp_season_interaction temp_season_category  weekend_temp_interaction  \\\n",
       "0                      9.0        Medium_Summer                       0.0   \n",
       "1                      9.0        Medium_Summer                       0.0   \n",
       "2                      9.0        Medium_Summer                       0.0   \n",
       "3                      9.0        Medium_Summer                       0.0   \n",
       "4                      9.0        Medium_Summer                       0.0   \n",
       "\n",
       "  weekend_temp_category  bewoelkung_weekend_interaction  \\\n",
       "0        Weekday_Medium                             0.0   \n",
       "1        Weekday_Medium                             0.0   \n",
       "2        Weekday_Medium                             0.0   \n",
       "3        Weekday_Medium                             0.0   \n",
       "4        Weekday_Medium                             0.0   \n",
       "\n",
       "  bewoelkung_weekend_category  \n",
       "0         Weekday_Very cloudy  \n",
       "1         Weekday_Very cloudy  \n",
       "2         Weekday_Very cloudy  \n",
       "3         Weekday_Very cloudy  \n",
       "4         Weekday_Very cloudy  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-split datasets\n",
    "data_path = '/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/'\n",
    "\n",
    "train_data = pd.read_csv(data_path + 'train_data.csv')\n",
    "val_data = pd.read_csv(data_path + 'val_data.csv')\n",
    "test_data = pd.read_csv(data_path + 'test_data.csv')\n",
    "\n",
    "# Convert date columns\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "val_data['date'] = pd.to_datetime(val_data['date'])\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä TRAINING SET:\")\n",
    "print(f\"   Shape: {train_data.shape[0]:,} rows √ó {train_data.shape[1]} columns\")\n",
    "print(f\"   Date range: {train_data['date'].min().date()} to {train_data['date'].max().date()}\")\n",
    "\n",
    "print(f\"\\nüìä VALIDATION SET:\")\n",
    "print(f\"   Shape: {val_data.shape[0]:,} rows √ó {val_data.shape[1]} columns\")\n",
    "print(f\"   Date range: {val_data['date'].min().date()} to {val_data['date'].max().date()}\")\n",
    "\n",
    "print(f\"\\nüìä TEST SET:\")\n",
    "print(f\"   Shape: {test_data.shape[0]:,} rows √ó {test_data.shape[1]} columns\")\n",
    "print(f\"   Date range: {test_data['date'].min().date()} to {test_data['date'].max().date()}\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2d327",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning & Preparation\n",
    "\n",
    "Using the same proven cleaning approach from the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c4cdddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "\n",
      "Training + Validation data: 9,334 rows\n",
      "Test data: 1,830 rows\n",
      "\n",
      "Dropping 9 lag features to prevent data leakage\n",
      "\n",
      "Applying time-aware imputation for weather features...\n",
      "  ‚úì Wettercode filled and synced to wettercode_category\n",
      "    Train Wettercode NaNs: 0\n",
      "    Train wettercode_category NaNs: 0\n",
      "  ‚úì Weather columns filled with forward-fill + month-based fallback\n",
      "  ‚úì Bewoelkung treated as Okta (0-9)\n",
      "  ‚úì Remaining columns filled with median/Unknown\n",
      "\n",
      "Remaining NaNs -> Training: 0, Test: 1830\n",
      "‚úì Data cleaned!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data...\\n\")\n",
    "\n",
    "# Combine train and val for cleaning\n",
    "df_with_sales = pd.concat([train_data, val_data], ignore_index=True)\n",
    "df_without_sales = test_data.copy()\n",
    "\n",
    "print(f\"Training + Validation data: {len(df_with_sales):,} rows\")\n",
    "print(f\"Test data: {len(df_without_sales):,} rows\")\n",
    "\n",
    "# Drop lag features (prevent data leakage)\n",
    "lag_cols = [col for col in df_with_sales.columns if any(x in col.lower() for x in ['lag', '_ma', 'volatility', 'pct_change', 'sum_7d'])]\n",
    "if 'id' in df_with_sales.columns:\n",
    "    lag_cols.append('id')\n",
    "\n",
    "print(f\"\\nDropping {len(lag_cols)} lag features to prevent data leakage\")\n",
    "\n",
    "df_with_sales = df_with_sales.drop(columns=lag_cols, errors='ignore')\n",
    "df_without_sales = df_without_sales.drop(columns=lag_cols, errors='ignore')\n",
    "\n",
    "# === Time-aware imputation (uses only past info, no future peeking) ===\n",
    "print(\"\\nApplying time-aware imputation for weather features...\")\n",
    "\n",
    "# Ensure month column exists for fallback\n",
    "if 'month' not in df_with_sales.columns:\n",
    "    df_with_sales['month'] = df_with_sales['date'].dt.month\n",
    "if 'month' not in df_without_sales.columns:\n",
    "    df_without_sales['month'] = df_without_sales['date'].dt.month\n",
    "\n",
    "# Identify weather columns\n",
    "numeric_weather_cols = [c for c in ['Temperatur', 'Niederschlag', 'Windgeschwindigkeit', \n",
    "                                     'Luftdruck', 'Luftfeuchte', 'Sonnenscheindauer', 'Bewoelkung'] \n",
    "                        if c in df_with_sales.columns]\n",
    "has_wettercode_cat = 'wettercode_category' in df_with_sales.columns\n",
    "has_wettercode_num = 'Wettercode' in df_with_sales.columns\n",
    "\n",
    "# Helper: sort by Warengruppe + date for proper time order\n",
    "group_col = 'Warengruppe' if 'Warengruppe' in df_with_sales.columns else None\n",
    "\n",
    "def sort_temporal(df_):\n",
    "    if group_col and group_col in df_.columns:\n",
    "        return df_.sort_values([group_col, 'date'])\n",
    "    return df_.sort_values('date')\n",
    "\n",
    "def ffill_by_group(df_, col):\n",
    "    if group_col and group_col in df_.columns:\n",
    "        df_[col] = df_.groupby(group_col, observed=True)[col].ffill()\n",
    "    else:\n",
    "        df_[col] = df_[col].ffill()\n",
    "\n",
    "# Sort both dataframes\n",
    "df_with_sales = sort_temporal(df_with_sales)\n",
    "df_without_sales = sort_temporal(df_without_sales)\n",
    "\n",
    "# 1) Numeric weather: forward fill, then per-month median fallback (computed on training data)\n",
    "for col in numeric_weather_cols:\n",
    "    # Ensure numeric\n",
    "    df_with_sales[col] = pd.to_numeric(df_with_sales[col], errors='coerce')\n",
    "    df_without_sales[col] = pd.to_numeric(df_without_sales[col], errors='coerce')\n",
    "    \n",
    "    # Forward fill by group\n",
    "    ffill_by_group(df_with_sales, col)\n",
    "    ffill_by_group(df_without_sales, col)\n",
    "    \n",
    "    # Month-based fallback using training data statistics\n",
    "    month_medians = df_with_sales.groupby('month')[col].median()\n",
    "    global_median = df_with_sales[col].median()\n",
    "    \n",
    "    for df_ in [df_with_sales, df_without_sales]:\n",
    "        missing = df_[col].isna()\n",
    "        if missing.any():\n",
    "            df_.loc[missing, col] = df_.loc[missing, 'month'].map(month_medians)\n",
    "            still_missing = df_[col].isna()\n",
    "            if still_missing.any():\n",
    "                df_.loc[still_missing, col] = global_median\n",
    "    \n",
    "    # Special: Bewoelkung is Okta (0-9), round and clip\n",
    "    if col == 'Bewoelkung':\n",
    "        df_with_sales[col] = np.round(df_with_sales[col]).clip(0, 9)\n",
    "        df_without_sales[col] = np.round(df_without_sales[col]).clip(0, 9)\n",
    "        # Convert to int if no NaNs left\n",
    "        if df_with_sales[col].isna().sum() == 0:\n",
    "            df_with_sales[col] = df_with_sales[col].astype(int)\n",
    "        if df_without_sales[col].isna().sum() == 0:\n",
    "            df_without_sales[col] = df_without_sales[col].astype(int)\n",
    "\n",
    "# 2) Wettercode: handle numeric and categorical in sync\n",
    "if has_wettercode_num or has_wettercode_cat:\n",
    "    # WMO code mapping\n",
    "    wmo_map = {\n",
    "        0.0: \"Cloud development not observed or not observable\",\n",
    "        3.0: \"Clouds generally forming or developing\",\n",
    "        5.0: \"Haze\",\n",
    "        10.0: \"Mist\",\n",
    "        17.0: \"Thunderstorm, no precipitation at observation\",\n",
    "        20.0: \"Drizzle (not freezing) or snow grains\",\n",
    "        21.0: \"Rain (not freezing)\",\n",
    "        22.0: \"Snow\",\n",
    "        28.0: \"Fog or ice fog\",\n",
    "        45.0: \"Fog or ice fog, sky invisible\",\n",
    "        49.0: \"Fog, depositing rime, sky invisible\",\n",
    "        53.0: \"Moderate drizzle, not freezing, continuous\",\n",
    "        55.0: \"Heavy drizzle, not freezing, continuous\",\n",
    "        61.0: \"Slight rain, not freezing, intermittent\",\n",
    "        63.0: \"Moderate rain, not freezing, continuous\",\n",
    "        65.0: \"Heavy rain, not freezing, continuous\",\n",
    "        68.0: \"Rain or drizzle and snow, slight\",\n",
    "        69.0: \"Rain or drizzle and snow, moderate or heavy\",\n",
    "        71.0: \"Slight continuous fall of snowflakes\",\n",
    "        73.0: \"Moderate continuous fall of snowflakes\",\n",
    "        75.0: \"Heavy continuous fall of snowflakes\",\n",
    "        77.0: \"Snow grains (with or without fog)\",\n",
    "        79.0: \"Ice pellets\",\n",
    "        95.0: \"Thunderstorm, slight/moderate, no hail but rain/snow\"\n",
    "    }\n",
    "    \n",
    "    # Ensure numeric\n",
    "    df_with_sales['Wettercode'] = pd.to_numeric(df_with_sales['Wettercode'], errors='coerce')\n",
    "    df_without_sales['Wettercode'] = pd.to_numeric(df_without_sales['Wettercode'], errors='coerce')\n",
    "    \n",
    "    # Forward fill by group\n",
    "    ffill_by_group(df_with_sales, 'Wettercode')\n",
    "    ffill_by_group(df_without_sales, 'Wettercode')\n",
    "    \n",
    "    # Month mode fallback (using only observed values)\n",
    "    month_modes = df_with_sales.groupby('month')['Wettercode'].apply(\n",
    "        lambda x: x.dropna().mode().iloc[0] if len(x.dropna().mode()) > 0 else None\n",
    "    )\n",
    "    global_mode = df_with_sales['Wettercode'].dropna().mode()\n",
    "    global_mode = global_mode.iloc[0] if len(global_mode) > 0 else 0.0  # Default to clear sky instead of rain\n",
    "    \n",
    "    for df_ in [df_with_sales, df_without_sales]:\n",
    "        missing = df_['Wettercode'].isna()\n",
    "        if missing.any():\n",
    "            df_.loc[missing, 'Wettercode'] = df_.loc[missing, 'month'].map(month_modes)\n",
    "            still_missing = df_['Wettercode'].isna()\n",
    "            if still_missing.any():\n",
    "                df_.loc[still_missing, 'Wettercode'] = global_mode\n",
    "    \n",
    "    # NOW sync wettercode_category from the (possibly filled) Wettercode\n",
    "    df_with_sales['wettercode_category'] = df_with_sales['Wettercode'].map(wmo_map)\n",
    "    df_without_sales['wettercode_category'] = df_without_sales['Wettercode'].map(wmo_map)\n",
    "    \n",
    "    # Fill any still-missing categories with 'Unknown'\n",
    "    df_with_sales['wettercode_category'].fillna('Unknown', inplace=True)\n",
    "    df_without_sales['wettercode_category'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    print(f\"  ‚úì Wettercode filled and synced to wettercode_category\")\n",
    "    print(f\"    Train Wettercode NaNs: {df_with_sales['Wettercode'].isna().sum()}\")\n",
    "    print(f\"    Train wettercode_category NaNs: {df_with_sales['wettercode_category'].isna().sum()}\")\n",
    "\n",
    "# 3) Fill remaining non-weather columns with simpler logic\n",
    "for col in df_with_sales.columns:\n",
    "    # Check if EITHER dataset has missing values\n",
    "    has_missing = df_with_sales[col].isnull().sum() > 0 or df_without_sales[col].isnull().sum() > 0\n",
    "    \n",
    "    if has_missing:\n",
    "        # Skip if already handled above\n",
    "        if col in numeric_weather_cols or col in ['wettercode_category', 'Wettercode', 'date', 'month']:\n",
    "            continue\n",
    "        \n",
    "        if df_with_sales[col].dtype in ['float64', 'int64']:\n",
    "            fill_val = df_with_sales[col].median()\n",
    "            df_with_sales[col].fillna(fill_val, inplace=True)\n",
    "            df_without_sales[col].fillna(fill_val, inplace=True)\n",
    "        else:\n",
    "            fill_val = 'Unknown'\n",
    "            df_with_sales[col].fillna(fill_val, inplace=True)\n",
    "            df_without_sales[col].fillna(fill_val, inplace=True)\n",
    "\n",
    "# Report\n",
    "print(f\"  ‚úì Weather columns filled with forward-fill + month-based fallback\")\n",
    "print(f\"  ‚úì Bewoelkung treated as Okta (0-9)\")\n",
    "print(f\"  ‚úì Remaining columns filled with median/Unknown\")\n",
    "\n",
    "# Show remaining NaNs\n",
    "train_nans = df_with_sales.isnull().sum().sum()\n",
    "test_nans = df_without_sales.isnull().sum().sum()\n",
    "print(f\"\\nRemaining NaNs -> Training: {train_nans}, Test: {test_nans}\")\n",
    "\n",
    "print(f\"‚úì Data cleaned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59560064",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c7a50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA SPLITS\n",
      "================================================================================\n",
      "\n",
      "  Training:   7,493 rows | 2013-07-01 to 2017-07-31\n",
      "  Validation: 1,841 rows | 2017-08-01 to 2018-07-31\n",
      "  Test:       1,830 rows | 2018-08-01 to 2019-07-30\n"
     ]
    }
   ],
   "source": [
    "# Re-split after cleaning\n",
    "train_data = df_with_sales[df_with_sales['date'] < '2017-08-01'].copy()\n",
    "val_data = df_with_sales[df_with_sales['date'] >= '2017-08-01'].copy()\n",
    "test_data = df_without_sales.copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA SPLITS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n  Training:   {len(train_data):,} rows | {train_data['date'].min().date()} to {train_data['date'].max().date()}\")\n",
    "print(f\"  Validation: {len(val_data):,} rows | {val_data['date'].min().date()} to {val_data['date'].max().date()}\")\n",
    "print(f\"  Test:       {len(test_data):,} rows | {test_data['date'].min().date()} to {test_data['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6967dd3",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Strategy Selection\n",
    "\n",
    "We'll try different feature sets:\n",
    "- **Baseline**: The 57 features that scored 0.92918\n",
    "- **Extended**: Add more interaction features\n",
    "- **Selected**: Use feature selection to find optimal subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edd9e9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING STRATEGY\n",
      "================================================================================\n",
      "\n",
      "‚úì Core Numeric Features (2): ['Temperatur', 'Bewoelkung']\n",
      "‚úì Core Categorical Features (10): ['Warengruppe', 'day_of_the_week', 'month', 'is_weekend', 'Is_Holiday', 'Day_Before_Holiday', 'KielerWoche', 'wettercode_category', 'season', 'temperature_category']\n",
      "\n",
      "‚úì Subset created: 12 features + target\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING STRATEGY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Start with baseline features (proven successful)\n",
    "core_numeric = [\n",
    "    'Temperatur',\n",
    "    'Bewoelkung',\n",
    "    'Niederschlag',\n",
    "    'Luftdruck'\n",
    "]\n",
    "\n",
    "core_categorical = [\n",
    "    'Warengruppe',\n",
    "    'day_of_the_week',\n",
    "    'month',\n",
    "    'is_weekend',\n",
    "    'Is_Holiday',\n",
    "    'Day_Before_Holiday',\n",
    "    'KielerWoche',\n",
    "    'wettercode_category',\n",
    "    'season',\n",
    "    'temperature_category'\n",
    "]\n",
    "\n",
    "# Check which features exist\n",
    "core_numeric = [c for c in core_numeric if c in train_data.columns]\n",
    "core_categorical = [c for c in core_categorical if c in train_data.columns]\n",
    "\n",
    "print(f\"‚úì Core Numeric Features ({len(core_numeric)}): {core_numeric}\")\n",
    "print(f\"‚úì Core Categorical Features ({len(core_categorical)}): {core_categorical}\")\n",
    "\n",
    "# Create feature subsets\n",
    "core_cols = core_numeric + core_categorical + ['umsatz', 'date']\n",
    "\n",
    "train_subset = train_data[[c for c in core_cols if c in train_data.columns]].copy()\n",
    "val_subset = val_data[[c for c in core_cols if c in val_data.columns]].copy()\n",
    "test_subset = test_data[[c for c in core_cols if c in test_data.columns]].copy()\n",
    "\n",
    "print(f\"\\n‚úì Subset created: {train_subset.shape[1]-2} features + target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7941808",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff2d3294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding features...\n",
      "  Categorical columns to encode: 10\n",
      "\n",
      "‚úì Feature matrices created:\n",
      "  X_train: (7493, 59)\n",
      "  X_val:   (1841, 59)\n",
      "  X_test:  (1830, 59)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEncoding features...\")\n",
    "\n",
    "# One-hot encode categorical features\n",
    "exclude_cols = ['date', 'umsatz']\n",
    "categorical_cols = [c for c in core_categorical if c in train_subset.columns]\n",
    "\n",
    "print(f\"  Categorical columns to encode: {len(categorical_cols)}\")\n",
    "\n",
    "all_data = pd.concat([train_subset, val_subset, test_subset], ignore_index=True)\n",
    "all_encoded = pd.get_dummies(all_data, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# Split back\n",
    "train_encoded = all_encoded.iloc[:len(train_subset)].copy()\n",
    "val_encoded = all_encoded.iloc[len(train_subset):len(train_subset)+len(val_subset)].copy()\n",
    "test_encoded = all_encoded.iloc[len(train_subset)+len(val_subset):].copy()\n",
    "\n",
    "# Create feature matrices\n",
    "feature_cols = [col for col in train_encoded.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_encoded[feature_cols]\n",
    "X_val = val_encoded[feature_cols]\n",
    "X_test = test_encoded[feature_cols]\n",
    "\n",
    "y_train = train_encoded['umsatz']\n",
    "y_val = val_encoded['umsatz']\n",
    "\n",
    "# Fill any remaining NaNs\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "print(f\"\\n‚úì Feature matrices created:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73aa8a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Baseline Model (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15fd041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BASELINE: Standard Linear Regression\n",
      "================================================================================\n",
      "\n",
      "üìä Validation Results:\n",
      "   MAPE:  32.76%\n",
      "   R¬≤:    0.7139\n",
      "   MAE:   ‚Ç¨49.55\n",
      "   RMSE:  ‚Ç¨69.58\n",
      "\n",
      "‚úì Baseline established (Kaggle reference: 0.92918)\n"
     ]
    }
   ],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    return mape\n",
    "\n",
    "def adjusted_r2(r2, n_samples, n_features):\n",
    "    \"\"\"Calculate Adjusted R¬≤\"\"\"\n",
    "    adj_r2 = 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)\n",
    "    return adj_r2\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE: Standard Linear Regression\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train baseline\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_baseline = lr_baseline.predict(X_val)\n",
    "\n",
    "# Metrics\n",
    "val_mape_baseline = calculate_mape(y_val, y_val_pred_baseline)\n",
    "val_r2_baseline = r2_score(y_val, y_val_pred_baseline)\n",
    "val_mae_baseline = mean_absolute_error(y_val, y_val_pred_baseline)\n",
    "val_rmse_baseline = np.sqrt(mean_squared_error(y_val, y_val_pred_baseline))\n",
    "\n",
    "print(f\"\\nüìä Validation Results:\")\n",
    "print(f\"   MAPE:  {val_mape_baseline:.2f}%\")\n",
    "print(f\"   R¬≤:    {val_r2_baseline:.4f}\")\n",
    "print(f\"   MAE:   ‚Ç¨{val_mae_baseline:.2f}\")\n",
    "print(f\"   RMSE:  ‚Ç¨{val_rmse_baseline:.2f}\")\n",
    "print(f\"\\n‚úì Baseline established (Kaggle reference: 0.92918)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37d63f",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Improvement Strategy 1: Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge regression adds L2 penalty to prevent overfitting and handle multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55839141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATEGY 1: Ridge Regression (L2 Regularization)\n",
      "================================================================================\n",
      "\n",
      "Testing alpha values: [0.01, 0.1, 1, 10, 100, 1000]\n",
      "  Œ±=  0.01: R¬≤ = 0.7139\n",
      "  Œ±=  0.10: R¬≤ = 0.7139\n",
      "  Œ±=  1.00: R¬≤ = 0.7139\n",
      "  Œ±= 10.00: R¬≤ = 0.7143\n",
      "  Œ±=100.00: R¬≤ = 0.7180\n",
      "  Œ±=1000.00: R¬≤ = 0.7272\n",
      "\n",
      "üèÜ Best Ridge Model (Œ±=1000):\n",
      "   MAPE:  32.39% (baseline: 32.76%)\n",
      "   R¬≤:    0.7272 (baseline: 0.7139)\n",
      "   MAE:   ‚Ç¨47.06 (baseline: ‚Ç¨49.55)\n",
      "\n",
      "‚úÖ R¬≤ change: +1.87%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY 1: Ridge Regression (L2 Regularization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Standardize features (important for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Grid search for best alpha\n",
    "alphas = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "best_score = -np.inf\n",
    "best_alpha = None\n",
    "\n",
    "print(f\"\\nTesting alpha values: {alphas}\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    y_val_pred = ridge.predict(X_val_scaled)\n",
    "    score = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_alpha = alpha\n",
    "    \n",
    "    print(f\"  Œ±={alpha:6.2f}: R¬≤ = {score:.4f}\")\n",
    "\n",
    "# Train with best alpha\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "y_val_pred_ridge = ridge_best.predict(X_val_scaled)\n",
    "\n",
    "# Metrics\n",
    "val_mape_ridge = calculate_mape(y_val, y_val_pred_ridge)\n",
    "val_r2_ridge = r2_score(y_val, y_val_pred_ridge)\n",
    "val_mae_ridge = mean_absolute_error(y_val, y_val_pred_ridge)\n",
    "\n",
    "print(f\"\\nüèÜ Best Ridge Model (Œ±={best_alpha}):\")\n",
    "print(f\"   MAPE:  {val_mape_ridge:.2f}% (baseline: {val_mape_baseline:.2f}%)\")\n",
    "print(f\"   R¬≤:    {val_r2_ridge:.4f} (baseline: {val_r2_baseline:.4f})\")\n",
    "print(f\"   MAE:   ‚Ç¨{val_mae_ridge:.2f} (baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "\n",
    "improvement = ((val_r2_ridge - val_r2_baseline) / val_r2_baseline) * 100\n",
    "print(f\"\\n{'‚úÖ' if improvement > 0 else '‚ùå'} R¬≤ change: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1bd7f",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Improvement Strategy 2: Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso performs feature selection by shrinking less important coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df6ea5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATEGY 2: Lasso Regression (L1 Regularization + Feature Selection)\n",
      "================================================================================\n",
      "\n",
      "Testing alpha values: [0.01, 0.1, 1, 10, 50]\n",
      "  Œ±=  0.01: R¬≤ = 0.7140 | Features selected: 51/59\n",
      "  Œ±=  0.10: R¬≤ = 0.7148 | Features selected: 48/59\n",
      "  Œ±=  1.00: R¬≤ = 0.7189 | Features selected: 33/59\n",
      "  Œ±= 10.00: R¬≤ = 0.6989 | Features selected: 8/59\n",
      "  Œ±= 50.00: R¬≤ = 0.3399 | Features selected: 1/59\n",
      "\n",
      "üèÜ Best Lasso Model (Œ±=1):\n",
      "   MAPE:  31.96% (baseline: 32.76%)\n",
      "   R¬≤:    0.7189 (baseline: 0.7139)\n",
      "   MAE:   ‚Ç¨48.39 (baseline: ‚Ç¨49.55)\n",
      "   Features: 33/59 selected\n",
      "\n",
      "‚úÖ R¬≤ change: +0.70%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY 2: Lasso Regression (L1 Regularization + Feature Selection)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Grid search for best alpha\n",
    "alphas = [0.01, 0.1, 1, 10, 50]\n",
    "best_score = -np.inf\n",
    "best_alpha = None\n",
    "\n",
    "print(f\"\\nTesting alpha values: {alphas}\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    y_val_pred = lasso.predict(X_val_scaled)\n",
    "    score = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    n_features = np.sum(lasso.coef_ != 0)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_alpha = alpha\n",
    "    \n",
    "    print(f\"  Œ±={alpha:6.2f}: R¬≤ = {score:.4f} | Features selected: {n_features}/{len(feature_cols)}\")\n",
    "\n",
    "# Train with best alpha\n",
    "lasso_best = Lasso(alpha=best_alpha, max_iter=10000)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "y_val_pred_lasso = lasso_best.predict(X_val_scaled)\n",
    "\n",
    "# Metrics\n",
    "val_mape_lasso = calculate_mape(y_val, y_val_pred_lasso)\n",
    "val_r2_lasso = r2_score(y_val, y_val_pred_lasso)\n",
    "val_mae_lasso = mean_absolute_error(y_val, y_val_pred_lasso)\n",
    "\n",
    "n_selected = np.sum(lasso_best.coef_ != 0)\n",
    "\n",
    "print(f\"\\nüèÜ Best Lasso Model (Œ±={best_alpha}):\")\n",
    "print(f\"   MAPE:  {val_mape_lasso:.2f}% (baseline: {val_mape_baseline:.2f}%)\")\n",
    "print(f\"   R¬≤:    {val_r2_lasso:.4f} (baseline: {val_r2_baseline:.4f})\")\n",
    "print(f\"   MAE:   ‚Ç¨{val_mae_lasso:.2f} (baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "print(f\"   Features: {n_selected}/{len(feature_cols)} selected\")\n",
    "\n",
    "improvement = ((val_r2_lasso - val_r2_baseline) / val_r2_baseline) * 100\n",
    "print(f\"\\n{'‚úÖ' if improvement > 0 else '‚ùå'} R¬≤ change: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16dfb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Improvement Strategy 3: ElasticNet (L1 + L2)\n",
    "\n",
    "ElasticNet combines Ridge and Lasso for balanced regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ffc4870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATEGY 3: ElasticNet (L1 + L2 Regularization)\n",
      "================================================================================\n",
      "\n",
      "Testing combinations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Best parameters: Œ±=1, l1_ratio=0.9\n",
      "\n",
      "üèÜ Best ElasticNet Model:\n",
      "   MAPE:  31.93% (baseline: 32.76%)\n",
      "   R¬≤:    0.7278 (baseline: 0.7139)\n",
      "   MAE:   ‚Ç¨46.77 (baseline: ‚Ç¨49.55)\n",
      "\n",
      "‚úÖ R¬≤ change: +1.96%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY 3: ElasticNet (L1 + L2 Regularization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Grid search\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "print(f\"\\nTesting combinations...\")\n",
    "\n",
    "for alpha in param_grid['alpha']:\n",
    "    for l1_ratio in param_grid['l1_ratio']:\n",
    "        enet = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n",
    "        enet.fit(X_train_scaled, y_train)\n",
    "        y_val_pred = enet.predict(X_val_scaled)\n",
    "        score = r2_score(y_val, y_val_pred)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'alpha': alpha, 'l1_ratio': l1_ratio}\n",
    "\n",
    "print(f\"\\nüéØ Best parameters: Œ±={best_params['alpha']}, l1_ratio={best_params['l1_ratio']}\")\n",
    "\n",
    "# Train with best params\n",
    "enet_best = ElasticNet(alpha=best_params['alpha'], l1_ratio=best_params['l1_ratio'], max_iter=10000)\n",
    "enet_best.fit(X_train_scaled, y_train)\n",
    "y_val_pred_enet = enet_best.predict(X_val_scaled)\n",
    "\n",
    "# Metrics\n",
    "val_mape_enet = calculate_mape(y_val, y_val_pred_enet)\n",
    "val_r2_enet = r2_score(y_val, y_val_pred_enet)\n",
    "val_mae_enet = mean_absolute_error(y_val, y_val_pred_enet)\n",
    "\n",
    "print(f\"\\nüèÜ Best ElasticNet Model:\")\n",
    "print(f\"   MAPE:  {val_mape_enet:.2f}% (baseline: {val_mape_baseline:.2f}%)\")\n",
    "print(f\"   R¬≤:    {val_r2_enet:.4f} (baseline: {val_r2_baseline:.4f})\")\n",
    "print(f\"   MAE:   ‚Ç¨{val_mae_enet:.2f} (baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "\n",
    "improvement = ((val_r2_enet - val_r2_baseline) / val_r2_baseline) * 100\n",
    "print(f\"\\n{'‚úÖ' if improvement > 0 else '‚ùå'} R¬≤ change: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43912d7a",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "945be779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "        Model  MAPE (%)       R¬≤   MAE (‚Ç¨)\n",
      "   ElasticNet 31.926563 0.727837 46.773879\n",
      "        Ridge 32.385945 0.727195 47.062032\n",
      "        Lasso 31.956728 0.718860 48.392556\n",
      "Baseline (LR) 32.759001 0.713880 49.550300\n",
      "\n",
      "üèÜ WINNER: ElasticNet (R¬≤ = 0.7278)\n",
      "\n",
      "üìä Reference: Simplified Baseline scored 0.92918 on Kaggle\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Baseline (LR)', 'Ridge', 'Lasso', 'ElasticNet'],\n",
    "    'MAPE (%)': [val_mape_baseline, val_mape_ridge, val_mape_lasso, val_mape_enet],\n",
    "    'R¬≤': [val_r2_baseline, val_r2_ridge, val_r2_lasso, val_r2_enet],\n",
    "    'MAE (‚Ç¨)': [val_mae_baseline, val_mae_ridge, val_mae_lasso, val_mae_enet]\n",
    "})\n",
    "\n",
    "results = results.sort_values('R¬≤', ascending=False)\n",
    "\n",
    "print(\"\\n\" + results.to_string(index=False))\n",
    "\n",
    "best_model_name = results.iloc[0]['Model']\n",
    "best_r2 = results.iloc[0]['R¬≤']\n",
    "\n",
    "print(f\"\\nüèÜ WINNER: {best_model_name} (R¬≤ = {best_r2:.4f})\")\n",
    "print(f\"\\nüìä Reference: Simplified Baseline scored 0.92918 on Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0f0ff",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Select Best Model & Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e951706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL MODEL SELECTION\n",
      "================================================================================\n",
      "\n",
      "üéØ Selected Model: ElasticNet\n",
      "   Validation R¬≤: 0.7278\n",
      "   Validation MAPE: 31.93%\n",
      "\n",
      "üìä Test Predictions:\n",
      "   Total: 1,830\n",
      "   Mean: ‚Ç¨190.34\n",
      "   Min:  ‚Ç¨48.67\n",
      "   Max:  ‚Ç¨394.67\n",
      "\n",
      "First 10 predictions:\n",
      "          id      umsatz\n",
      "0  1808011.0  144.100861\n",
      "1  1808012.0  146.763676\n",
      "2  1808013.0  147.907862\n",
      "3  1808015.0  143.708847\n",
      "4  1808014.0  138.068618\n",
      "5  1808022.0  143.373391\n",
      "6  1808023.0  153.883470\n",
      "7  1808024.0  147.016777\n",
      "8  1808025.0  142.615719\n",
      "9  1808021.0  134.654726\n",
      "\n",
      "‚úì Saved to: advanced_lr_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select best model based on R¬≤\n",
    "models = {\n",
    "    'Baseline (LR)': (lr_baseline, False),  # (model, needs_scaling)\n",
    "    'Ridge': (ridge_best, True),\n",
    "    'Lasso': (lasso_best, True),\n",
    "    'ElasticNet': (enet_best, True)\n",
    "}\n",
    "\n",
    "best_idx = results['R¬≤'].idxmax()\n",
    "best_model_name = results.loc[best_idx, 'Model']\n",
    "best_model, needs_scaling = models[best_model_name]\n",
    "\n",
    "print(f\"\\nüéØ Selected Model: {best_model_name}\")\n",
    "print(f\"   Validation R¬≤: {results.loc[best_idx, 'R¬≤']:.4f}\")\n",
    "print(f\"   Validation MAPE: {results.loc[best_idx, 'MAPE (%)']:.2f}%\")\n",
    "\n",
    "# Generate test predictions\n",
    "if needs_scaling:\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    test_predictions = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Load test_data.csv to get correct IDs\n",
    "test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "\n",
    "# Create Kaggle submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_with_ids['id'].values,\n",
    "    'umsatz': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Test Predictions:\")\n",
    "print(f\"   Total: {len(submission):,}\")\n",
    "print(f\"   Mean: ‚Ç¨{submission['umsatz'].mean():.2f}\")\n",
    "print(f\"   Min:  ‚Ç¨{submission['umsatz'].min():.2f}\")\n",
    "print(f\"   Max:  ‚Ç¨{submission['umsatz'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Save\n",
    "submission.to_csv('advanced_lr_predictions.csv', index=False)\n",
    "print(f\"\\n‚úì Saved to: advanced_lr_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3ed04",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Next Steps & Recommendations\n",
    "\n",
    "### If results improved:\n",
    "1. ‚úÖ Submit to Kaggle and compare with baseline (0.92918)\n",
    "2. üîÑ Try polynomial features for non-linear relationships\n",
    "3. üéØ Feature engineering: create interaction terms\n",
    "4. üìä Ensemble: combine multiple models\n",
    "\n",
    "### If results didn't improve:\n",
    "1. üîç The baseline features may already be optimal\n",
    "2. üí° Try adding domain-specific features\n",
    "3. üå°Ô∏è Weather interactions with product type\n",
    "4. üìÖ More sophisticated time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "124e55d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéâ ADVANCED LINEAR REGRESSION MODEL COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìà Summary:\n",
      "   Best Model: ElasticNet\n",
      "   Validation R¬≤: 0.7278\n",
      "   Baseline Reference: 0.92918 (Kaggle)\n",
      "\n",
      "üéØ Next Action: Submit advanced_lr_predictions.csv to Kaggle!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéâ ADVANCED LINEAR REGRESSION MODEL COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìà Summary:\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   Validation R¬≤: {results.loc[best_idx, 'R¬≤']:.4f}\")\n",
    "print(f\"   Baseline Reference: 0.92918 (Kaggle)\")\n",
    "\n",
    "print(f\"\\nüéØ Next Action: Submit advanced_lr_predictions.csv to Kaggle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87678dd",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Alternative Strategy: Feature Interactions\n",
    "\n",
    "**Kaggle Result: Regularization got 0.89738 (worse than baseline 0.92918)**\n",
    "\n",
    "Since regularization hurt performance, the baseline isn't overfitting. Let's try feature interactions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5b7da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATEGY: Feature Interactions\n",
      "================================================================================\n",
      "\n",
      "Creating interaction features...\n",
      "‚úì Interactions added!\n",
      "  New shape: (7493, 28)\n",
      "\n",
      "Encoding 10 categorical features...\n",
      "\n",
      "‚úì Feature matrices with interactions:\n",
      "  X_train: (7493, 73) (baseline: (7493, 59))\n",
      "  X_val:   (1841, 73)\n",
      "  X_test:  (1830, 73)\n",
      "  Added 14 interaction features\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY: Feature Interactions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Go back to original cleaned data (before encoding)\n",
    "train_interact = train_data[[c for c in core_numeric + core_categorical + ['umsatz', 'date'] if c in train_data.columns]].copy()\n",
    "val_interact = val_data[[c for c in core_numeric + core_categorical + ['umsatz', 'date'] if c in val_data.columns]].copy()\n",
    "test_interact = test_data[[c for c in core_numeric + core_categorical + ['umsatz', 'date'] if c in test_data.columns]].copy()\n",
    "\n",
    "print(f\"\\nCreating interaction features...\")\n",
    "\n",
    "# Function to add interactions\n",
    "def add_interactions(df):\n",
    "    \"\"\"Add domain-specific feature interactions\"\"\"\n",
    "    \n",
    "    # Helper function to safely convert to binary (handles 'Unknown' and other non-numeric values)\n",
    "    def safe_binary(series):\n",
    "        \"\"\"Convert to 0/1, treating 'Unknown' and other non-numeric as 0\"\"\"\n",
    "        if series.dtype in ['int64', 'float64']:\n",
    "            return series.astype(int)\n",
    "        else:\n",
    "            # For object/string types, convert True/1/'1' to 1, everything else to 0\n",
    "            return series.apply(lambda x: 1 if x in [True, 1, '1', 1.0] else 0)\n",
    "    \n",
    "    # 1. Temperature √ó Season (cold affects different seasons differently)\n",
    "    if 'Temperatur' in df.columns and 'season' in df.columns:\n",
    "        for season in df['season'].unique():\n",
    "            if season != 'Unknown':\n",
    "                df[f'temp_in_{season}'] = df['Temperatur'] * (df['season'] == season).astype(int)\n",
    "    \n",
    "    # 2. Weather √ó Weekend (weekend weather impact)\n",
    "    if 'Temperatur' in df.columns and 'is_weekend' in df.columns:\n",
    "        df['temp_weekend'] = df['Temperatur'] * safe_binary(df['is_weekend'])\n",
    "    \n",
    "    if 'Niederschlag' in df.columns and 'is_weekend' in df.columns:\n",
    "        df['rain_weekend'] = df['Niederschlag'] * safe_binary(df['is_weekend'])\n",
    "    \n",
    "    # 3. Product √ó Day features (different products on different days)\n",
    "    if 'Warengruppe' in df.columns and 'day_of_the_week' in df.columns:\n",
    "        # One-hot then multiply isn't needed, just encode together later\n",
    "        pass\n",
    "    \n",
    "    # 4. Temperature category √ó actual temperature (capture within-category variation)\n",
    "    if 'Temperatur' in df.columns and 'temperature_category' in df.columns:\n",
    "        for temp_cat in df['temperature_category'].unique():\n",
    "            if temp_cat != 'Unknown':\n",
    "                df[f'temp_actual_in_{temp_cat}'] = df['Temperatur'] * (df['temperature_category'] == temp_cat).astype(int)\n",
    "    \n",
    "    # 5. Holiday √ó Weather (holidays affected by weather)\n",
    "    if 'Temperatur' in df.columns and 'Is_Holiday' in df.columns:\n",
    "        df['temp_holiday'] = df['Temperatur'] * safe_binary(df['Is_Holiday'])\n",
    "    \n",
    "    if 'Niederschlag' in df.columns and 'Is_Holiday' in df.columns:\n",
    "        df['rain_holiday'] = df['Niederschlag'] * safe_binary(df['Is_Holiday'])\n",
    "    \n",
    "    # 6. KielerWoche √ó Weather (special event weather impact)\n",
    "    if 'Temperatur' in df.columns and 'KielerWoche' in df.columns:\n",
    "        df['temp_kielerwoche'] = df['Temperatur'] * safe_binary(df['KielerWoche'])\n",
    "    \n",
    "    # 7. Month √ó Temperature (seasonal temperature patterns)\n",
    "    if 'Temperatur' in df.columns and 'month' in df.columns:\n",
    "        # Create interaction for summer months\n",
    "        df['temp_summer_months'] = df['Temperatur'] * df['month'].isin([6, 7, 8]).astype(int)\n",
    "        df['temp_winter_months'] = df['Temperatur'] * df['month'].isin([12, 1, 2]).astype(int)\n",
    "    \n",
    "    # 8. Pressure √ó Rain (high pressure = less rain correlation)\n",
    "    if 'Luftdruck' in df.columns and 'Niederschlag' in df.columns:\n",
    "        df['pressure_rain_interaction'] = df['Luftdruck'] * df['Niederschlag']\n",
    "    \n",
    "    # 9. Temperature √ó Cloud cover\n",
    "    if 'Temperatur' in df.columns and 'Bewoelkung' in df.columns:\n",
    "        df['temp_clouds'] = df['Temperatur'] * df['Bewoelkung']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add interactions to all datasets\n",
    "train_interact = add_interactions(train_interact)\n",
    "val_interact = add_interactions(val_interact)\n",
    "test_interact = add_interactions(test_interact)\n",
    "\n",
    "print(f\"‚úì Interactions added!\")\n",
    "print(f\"  New shape: {train_interact.shape}\")\n",
    "\n",
    "# Now encode everything\n",
    "exclude_cols = ['date', 'umsatz']\n",
    "categorical_cols = [c for c in core_categorical if c in train_interact.columns]\n",
    "\n",
    "print(f\"\\nEncoding {len(categorical_cols)} categorical features...\")\n",
    "\n",
    "all_interact = pd.concat([train_interact, val_interact, test_interact], ignore_index=True)\n",
    "all_interact_encoded = pd.get_dummies(all_interact, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# Split back\n",
    "train_interact_encoded = all_interact_encoded.iloc[:len(train_interact)].copy()\n",
    "val_interact_encoded = all_interact_encoded.iloc[len(train_interact):len(train_interact)+len(val_interact)].copy()\n",
    "test_interact_encoded = all_interact_encoded.iloc[len(train_interact)+len(val_interact):].copy()\n",
    "\n",
    "# Create feature matrices\n",
    "feature_cols_interact = [col for col in train_interact_encoded.columns if col not in exclude_cols]\n",
    "\n",
    "X_train_interact = train_interact_encoded[feature_cols_interact]\n",
    "X_val_interact = val_interact_encoded[feature_cols_interact]\n",
    "X_test_interact = test_interact_encoded[feature_cols_interact]\n",
    "\n",
    "y_train_interact = train_interact_encoded['umsatz']\n",
    "y_val_interact = val_interact_encoded['umsatz']\n",
    "\n",
    "# Fill NaNs\n",
    "X_train_interact = X_train_interact.fillna(0)\n",
    "X_val_interact = X_val_interact.fillna(0)\n",
    "X_test_interact = X_test_interact.fillna(0)\n",
    "\n",
    "print(f\"\\n‚úì Feature matrices with interactions:\")\n",
    "print(f\"  X_train: {X_train_interact.shape} (baseline: {X_train.shape})\")\n",
    "print(f\"  X_val:   {X_val_interact.shape}\")\n",
    "print(f\"  X_test:  {X_test_interact.shape}\")\n",
    "print(f\"  Added {X_train_interact.shape[1] - X_train.shape[1]} interaction features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41587b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Linear Regression with Interactions\n",
      "================================================================================\n",
      "\n",
      "üìä Results with Interactions:\n",
      "   MAPE:  32.77% (baseline: 32.76%)\n",
      "   R¬≤:    0.7129 (baseline: 0.7139)\n",
      "   MAE:   ‚Ç¨49.54 (baseline: ‚Ç¨49.55)\n",
      "\n",
      "‚ùå R¬≤ change: -0.13%\n",
      "\n",
      "‚ö†Ô∏è  Interactions didn't improve validation performance\n",
      "   Try other strategies below...\n"
     ]
    }
   ],
   "source": [
    "# Train model with interactions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Linear Regression with Interactions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lr_interact = LinearRegression()\n",
    "lr_interact.fit(X_train_interact, y_train_interact)\n",
    "\n",
    "y_val_pred_interact = lr_interact.predict(X_val_interact)\n",
    "\n",
    "# Metrics\n",
    "val_mape_interact = calculate_mape(y_val_interact, y_val_pred_interact)\n",
    "val_r2_interact = r2_score(y_val_interact, y_val_pred_interact)\n",
    "val_mae_interact = mean_absolute_error(y_val_interact, y_val_pred_interact)\n",
    "\n",
    "print(f\"\\nüìä Results with Interactions:\")\n",
    "print(f\"   MAPE:  {val_mape_interact:.2f}% (baseline: {val_mape_baseline:.2f}%)\")\n",
    "print(f\"   R¬≤:    {val_r2_interact:.4f} (baseline: {val_r2_baseline:.4f})\")\n",
    "print(f\"   MAE:   ‚Ç¨{val_mae_interact:.2f} (baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "\n",
    "improvement = ((val_r2_interact - val_r2_baseline) / val_r2_baseline) * 100\n",
    "print(f\"\\n{'‚úÖ' if improvement > 0 else '‚ùå'} R¬≤ change: {improvement:+.2f}%\")\n",
    "\n",
    "if val_r2_interact > val_r2_baseline:\n",
    "    print(f\"\\nüéâ IMPROVEMENT! Generating predictions...\")\n",
    "    \n",
    "    # Generate test predictions\n",
    "    test_predictions_interact = lr_interact.predict(X_test_interact)\n",
    "    \n",
    "    # Load test_data.csv for IDs\n",
    "    test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "    \n",
    "    # Create submission\n",
    "    submission_interact = pd.DataFrame({\n",
    "        'id': test_with_ids['id'].values,\n",
    "        'umsatz': test_predictions_interact\n",
    "    })\n",
    "    \n",
    "    submission_interact.to_csv('advanced_lr_predictions_interactions.csv', index=False)\n",
    "    print(f\"   ‚úì Saved: advanced_lr_predictions_interactions.csv\")\n",
    "    print(f\"   üì§ Submit this to Kaggle!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Interactions didn't improve validation performance\")\n",
    "    print(f\"   Try other strategies below...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9457e",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Alternative Strategy: Polynomial Features (Degree 2)\n",
    "\n",
    "If interactions don't help, try polynomial features to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e7db04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATEGY: Polynomial Features (Degree 2)\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  WARNING: This creates 59 √ó (59+1) / 2 features\n",
      "   Original features: 59\n",
      "   Estimated polynomial features: ~1770\n",
      "\n",
      "   This might be too many! Consider using only numeric features.\n",
      "\n",
      "   Using only 2 numeric features for polynomial\n",
      "\n",
      "‚úì Polynomial features created:\n",
      "  X_train: (7493, 62) (baseline: (7493, 59))\n",
      "  Added 3 polynomial terms\n",
      "\n",
      "   Training model...\n",
      "\n",
      "üìä Results with Polynomial Features:\n",
      "   MAPE:  32.71% (baseline: 32.76%)\n",
      "   R¬≤:    0.7134 (baseline: 0.7139)\n",
      "   MAE:   ‚Ç¨49.47 (baseline: ‚Ç¨49.55)\n",
      "\n",
      "‚ùå R¬≤ change: -0.06%\n",
      "\n",
      "‚ö†Ô∏è  Polynomial features didn't improve validation performance\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY: Polynomial Features (Degree 2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  WARNING: This creates {X_train.shape[1]} √ó ({X_train.shape[1]}+1) / 2 features\")\n",
    "print(f\"   Original features: {X_train.shape[1]}\")\n",
    "print(f\"   Estimated polynomial features: ~{X_train.shape[1] * (X_train.shape[1] + 1) // 2}\")\n",
    "print(f\"\\n   This might be too many! Consider using only numeric features.\")\n",
    "\n",
    "# Use only numeric features for polynomial to avoid explosion\n",
    "numeric_features = core_numeric\n",
    "X_train_numeric = train_encoded[[c for c in numeric_features if c in train_encoded.columns]]\n",
    "X_val_numeric = val_encoded[[c for c in numeric_features if c in val_encoded.columns]]\n",
    "X_test_numeric = test_encoded[[c for c in numeric_features if c in test_encoded.columns]]\n",
    "\n",
    "print(f\"\\n   Using only {len(numeric_features)} numeric features for polynomial\")\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_numeric)\n",
    "X_val_poly = poly.transform(X_val_numeric)\n",
    "X_test_poly = poly.transform(X_test_numeric)\n",
    "\n",
    "# Combine with categorical features\n",
    "categorical_features = [c for c in X_train.columns if c not in numeric_features]\n",
    "X_train_cat = X_train[categorical_features]\n",
    "X_val_cat = X_val[categorical_features]\n",
    "X_test_cat = X_test[categorical_features]\n",
    "\n",
    "# Concatenate\n",
    "X_train_poly_full = np.hstack([X_train_poly, X_train_cat.values])\n",
    "X_val_poly_full = np.hstack([X_val_poly, X_val_cat.values])\n",
    "X_test_poly_full = np.hstack([X_test_poly, X_test_cat.values])\n",
    "\n",
    "print(f\"\\n‚úì Polynomial features created:\")\n",
    "print(f\"  X_train: {X_train_poly_full.shape} (baseline: {X_train.shape})\")\n",
    "print(f\"  Added {X_train_poly_full.shape[1] - X_train.shape[1]} polynomial terms\")\n",
    "\n",
    "# Train\n",
    "print(f\"\\n   Training model...\")\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_train_poly_full, y_train)\n",
    "\n",
    "y_val_pred_poly = lr_poly.predict(X_val_poly_full)\n",
    "\n",
    "# Metrics\n",
    "val_mape_poly = calculate_mape(y_val, y_val_pred_poly)\n",
    "val_r2_poly = r2_score(y_val, y_val_pred_poly)\n",
    "val_mae_poly = mean_absolute_error(y_val, y_val_pred_poly)\n",
    "\n",
    "print(f\"\\nüìä Results with Polynomial Features:\")\n",
    "print(f\"   MAPE:  {val_mape_poly:.2f}% (baseline: {val_mape_baseline:.2f}%)\")\n",
    "print(f\"   R¬≤:    {val_r2_poly:.4f} (baseline: {val_r2_baseline:.4f})\")\n",
    "print(f\"   MAE:   ‚Ç¨{val_mae_poly:.2f} (baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "\n",
    "improvement = ((val_r2_poly - val_r2_baseline) / val_r2_baseline) * 100\n",
    "print(f\"\\n{'‚úÖ' if improvement > 0 else '‚ùå'} R¬≤ change: {improvement:+.2f}%\")\n",
    "\n",
    "if val_r2_poly > val_r2_baseline:\n",
    "    print(f\"\\nüéâ IMPROVEMENT! Generating predictions...\")\n",
    "    \n",
    "    test_predictions_poly = lr_poly.predict(X_test_poly_full)\n",
    "    \n",
    "    test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "    \n",
    "    submission_poly = pd.DataFrame({\n",
    "        'id': test_with_ids['id'].values,\n",
    "        'umsatz': test_predictions_poly\n",
    "    })\n",
    "    \n",
    "    submission_poly.to_csv('advanced_lr_predictions_polynomial.csv', index=False)\n",
    "    print(f\"   ‚úì Saved: advanced_lr_predictions_polynomial.csv\")\n",
    "    print(f\"   üì§ Submit this to Kaggle!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Polynomial features didn't improve validation performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d686c",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Summary & Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "- **Regularization (Ridge/Lasso/ElasticNet)**: Made it worse (0.89738) ‚Üí Baseline isn't overfitting!\n",
    "- **Feature Interactions**: Test if domain knowledge helps\n",
    "- **Polynomial Features**: Test for non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daacc2cf",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Strategy: Time & Group Signals + Outlier Trimming\n",
    "\n",
    "Add time-aware signals (week-of-year sine/cosine, quarter), group mean target signal, and trim outlier targets before fitting a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f9458ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATEGY: Time & Group Signals + Outlier Trimming\n",
      "================================================================================\n",
      "\n",
      "Encoding 10 categorical features...\n",
      "\n",
      "‚úì Time/Group feature matrices:\n",
      "  X_train_time: (7493, 64)\n",
      "  X_val_time:   (1841, 64)\n",
      "  X_test_time:  (1830, 64)\n",
      "\n",
      "üìä Results (Time & Group signals + trimmed target):\n",
      "   MAPE:  32.62% (baseline: 32.76%)\n",
      "   R¬≤:    0.7179 (baseline: 0.7139)\n",
      "   MAE:   ‚Ç¨49.02 (baseline: ‚Ç¨49.55)\n",
      "\n",
      "üéâ Improvement! Saved: advanced_lr_predictions_time.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY: Time & Group Signals + Outlier Trimming\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make fresh copies\n",
    "train_time = train_data.copy()\n",
    "val_time = val_data.copy()\n",
    "test_time = test_data.copy()\n",
    "\n",
    "# Time features\n",
    "pi = np.pi\n",
    "def add_time_feats(df):\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['week_sin'] = np.sin(2 * pi * df['weekofyear'] / 52)\n",
    "    df['week_cos'] = np.cos(2 * pi * df['weekofyear'] / 52)\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    return df\n",
    "\n",
    "for df in [train_time, val_time, test_time]:\n",
    "    add_time_feats(df)\n",
    "\n",
    "# School holiday flag (if available, else fallback to Is_Holiday)\n",
    "school_cols = [c for c in ['SchoolHoliday', 'School_Holiday', 'school_holiday'] if c in train_time.columns]\n",
    "if school_cols:\n",
    "    school_col = school_cols[0]\n",
    "    def to_flag(series):\n",
    "        if series.dtype in ['int64', 'float64']:\n",
    "            return series.fillna(0).astype(int).clip(0, 1)\n",
    "        return series.fillna('0').apply(lambda x: 1 if str(x) in ['1', 'True', 'true'] else 0)\n",
    "    for df in [train_time, val_time, test_time]:\n",
    "        df['school_holiday_flag'] = to_flag(df[school_col])\n",
    "elif 'Is_Holiday' in train_time.columns:\n",
    "    for df in [train_time, val_time, test_time]:\n",
    "        df['school_holiday_flag'] = df['Is_Holiday'].apply(lambda x: 1 if x in [1, True, '1'] else 0)\n",
    "\n",
    "# Warengruppe mean target encoding (train stats only)\n",
    "if 'Warengruppe' in train_time.columns:\n",
    "    wg_mean = train_time.groupby('Warengruppe')['umsatz'].mean()\n",
    "    global_mean = train_time['umsatz'].mean()\n",
    "    for df in [train_time, val_time, test_time]:\n",
    "        df['warengruppe_mean_sales'] = df['Warengruppe'].map(wg_mean).fillna(global_mean)\n",
    "else:\n",
    "    global_mean = train_time['umsatz'].mean()\n",
    "    for df in [train_time, val_time, test_time]:\n",
    "        df['warengruppe_mean_sales'] = global_mean\n",
    "\n",
    "# Winsorize training target\n",
    "p_low, p_high = np.percentile(train_time['umsatz'], [0.5, 99.5])\n",
    "y_train_time = train_time['umsatz'].clip(p_low, p_high)\n",
    "\n",
    "# Feature lists\n",
    "extra_numeric = ['week_sin', 'week_cos', 'quarter', 'warengruppe_mean_sales']\n",
    "if 'school_holiday_flag' in train_time.columns:\n",
    "    extra_numeric.append('school_holiday_flag')\n",
    "\n",
    "time_numeric = [c for c in core_numeric + extra_numeric if c in train_time.columns]\n",
    "time_categorical = [c for c in core_categorical if c in train_time.columns]\n",
    "\n",
    "core_cols_time = time_numeric + time_categorical + ['umsatz', 'date']\n",
    "train_time_subset = train_time[[c for c in core_cols_time if c in train_time.columns]].copy()\n",
    "val_time_subset = val_time[[c for c in core_cols_time if c in val_time.columns]].copy()\n",
    "test_time_subset = test_time[[c for c in core_cols_time if c in test_time.columns]].copy()\n",
    "\n",
    "# Encoding\n",
    "exclude_cols = ['date', 'umsatz']\n",
    "cat_cols_time = [c for c in time_categorical if c in train_time_subset.columns]\n",
    "\n",
    "print(f\"\\nEncoding {len(cat_cols_time)} categorical features...\")\n",
    "\n",
    "all_time = pd.concat([train_time_subset, val_time_subset, test_time_subset], ignore_index=True)\n",
    "all_time_encoded = pd.get_dummies(all_time, columns=cat_cols_time, drop_first=True, dtype=int)\n",
    "\n",
    "train_time_encoded = all_time_encoded.iloc[:len(train_time_subset)].copy()\n",
    "val_time_encoded = all_time_encoded.iloc[len(train_time_subset):len(train_time_subset)+len(val_time_subset)].copy()\n",
    "test_time_encoded = all_time_encoded.iloc[len(train_time_subset)+len(val_time_subset):].copy()\n",
    "\n",
    "feature_cols_time = [col for col in train_time_encoded.columns if col not in exclude_cols]\n",
    "\n",
    "X_train_time = train_time_encoded[feature_cols_time].fillna(0)\n",
    "X_val_time = val_time_encoded[feature_cols_time].fillna(0)\n",
    "X_test_time = test_time_encoded[feature_cols_time].fillna(0)\n",
    "\n",
    "y_val_time = val_time_encoded['umsatz']\n",
    "\n",
    "print(f\"\\n‚úì Time/Group feature matrices:\")\n",
    "print(f\"  X_train_time: {X_train_time.shape}\")\n",
    "print(f\"  X_val_time:   {X_val_time.shape}\")\n",
    "print(f\"  X_test_time:  {X_test_time.shape}\")\n",
    "\n",
    "# Train\n",
    "lr_time = LinearRegression()\n",
    "lr_time.fit(X_train_time, y_train_time)\n",
    "\n",
    "y_val_pred_time = lr_time.predict(X_val_time)\n",
    "\n",
    "time_mape = calculate_mape(y_val_time, y_val_pred_time)\n",
    "time_r2 = r2_score(y_val_time, y_val_pred_time)\n",
    "time_mae = mean_absolute_error(y_val_time, y_val_pred_time)\n",
    "\n",
    "print(f\"\\nüìä Results (Time & Group signals + trimmed target):\")\n",
    "print(f\"   MAPE:  {time_mape:.2f}% (baseline: {val_mape_baseline:.2f}%)\")\n",
    "print(f\"   R¬≤:    {time_r2:.4f} (baseline: {val_r2_baseline:.4f})\")\n",
    "print(f\"   MAE:   ‚Ç¨{time_mae:.2f} (baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "\n",
    "# Test predictions for blending/optionally saving\n",
    "test_predictions_time = lr_time.predict(X_test_time)\n",
    "\n",
    "if time_r2 > val_r2_baseline:\n",
    "    test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "    submission_time = pd.DataFrame({\n",
    "        'id': test_with_ids['id'].values,\n",
    "        'umsatz': test_predictions_time\n",
    "    })\n",
    "    submission_time.to_csv('advanced_lr_predictions_time.csv', index=False)\n",
    "    print(f\"\\nüéâ Improvement! Saved: advanced_lr_predictions_time.csv\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Time/Group model did not beat baseline; keeping for potential blending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4f270",
   "metadata": {},
   "source": [
    "---\n",
    "## 18. Strategy: Simple Blending (Ensemble)\n",
    "\n",
    "Blend baseline with other variants (interactions, time features, polynomial) to see if a weighted average improves validation R¬≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2db1d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATEGY: Simple Blending\n",
      "================================================================================\n",
      "\n",
      "Best blend candidate: baseline + time (weight on time: 0.8)\n",
      "   R¬≤: 0.7173 (baseline: 0.7139)\n",
      "   MAPE: 32.63% (baseline: 32.76%)\n",
      "   MAE: ‚Ç¨49.11 (baseline: ‚Ç¨49.55)\n",
      "\n",
      "üéâ Improvement via blend! Saved: advanced_lr_predictions_blend.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY: Simple Blending\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect validation predictions\n",
    "candidates = {}\n",
    "\n",
    "# Baseline (required)\n",
    "try:\n",
    "    baseline_val = y_val_pred_baseline\n",
    "    baseline_test = lr_baseline.predict(X_test)\n",
    "    candidates['baseline'] = {\n",
    "        'val': baseline_val,\n",
    "        'test': baseline_test\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(\"Baseline predictions missing; cannot blend\")\n",
    "\n",
    "# Interactions\n",
    "if 'y_val_pred_interact' in locals() and 'lr_interact' in locals():\n",
    "    try:\n",
    "        candidates['interact'] = {\n",
    "            'val': y_val_pred_interact,\n",
    "            'test': lr_interact.predict(X_test_interact)\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Polynomial\n",
    "if 'y_val_pred_poly' in locals() and 'lr_poly' in locals():\n",
    "    try:\n",
    "        candidates['poly'] = {\n",
    "            'val': y_val_pred_poly,\n",
    "            'test': lr_poly.predict(X_test_poly_full)\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Time/Group\n",
    "if 'y_val_pred_time' in locals() and 'lr_time' in locals():\n",
    "    try:\n",
    "        candidates['time'] = {\n",
    "            'val': y_val_pred_time,\n",
    "            'test': test_predictions_time\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if len(candidates) < 2:\n",
    "    print(\"Not enough models to blend; need at least baseline + 1 other model\")\n",
    "else:\n",
    "    weights = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    best = {\n",
    "        'r2': val_r2_baseline,\n",
    "        'mape': val_mape_baseline,\n",
    "        'mae': val_mae_baseline,\n",
    "        'name': 'baseline',\n",
    "        'weight': 0,\n",
    "        'partner': None\n",
    "    }\n",
    "    \n",
    "    for name, preds in candidates.items():\n",
    "        if name == 'baseline':\n",
    "            continue\n",
    "        for w in weights:\n",
    "            blend_val = w * preds['val'] + (1 - w) * candidates['baseline']['val']\n",
    "            r2 = r2_score(y_val, blend_val)\n",
    "            mape = calculate_mape(y_val, blend_val)\n",
    "            mae = mean_absolute_error(y_val, blend_val)\n",
    "            if r2 > best['r2']:\n",
    "                best = {\n",
    "                    'r2': r2,\n",
    "                    'mape': mape,\n",
    "                    'mae': mae,\n",
    "                    'name': name,\n",
    "                    'weight': w,\n",
    "                    'partner': 'baseline'\n",
    "                }\n",
    "    \n",
    "    print(f\"\\nBest blend candidate: {best['partner']} + {best['name']} (weight on {best['name']}: {best['weight']:.1f})\")\n",
    "    print(f\"   R¬≤: {best['r2']:.4f} (baseline: {val_r2_baseline:.4f})\")\n",
    "    print(f\"   MAPE: {best['mape']:.2f}% (baseline: {val_mape_baseline:.2f}%)\")\n",
    "    print(f\"   MAE: ‚Ç¨{best['mae']:.2f} (baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "    \n",
    "    if best['r2'] > val_r2_baseline:\n",
    "        test_pred_blend = best['weight'] * candidates[best['name']]['test'] + (1 - best['weight']) * candidates['baseline']['test']\n",
    "        test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "        submission_blend = pd.DataFrame({\n",
    "            'id': test_with_ids['id'].values,\n",
    "            'umsatz': test_pred_blend\n",
    "        })\n",
    "        submission_blend.to_csv('advanced_lr_predictions_blend.csv', index=False)\n",
    "        print(f\"\\nüéâ Improvement via blend! Saved: advanced_lr_predictions_blend.csv\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Blending did not beat baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5564c6a9",
   "metadata": {},
   "source": [
    "---\n",
    "## 19. Fine-Grained Blend: Time Model + Baseline\n",
    "\n",
    "Search finer weights between the strong time model and baseline to squeeze a small lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "103f274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINE BLEND: Time + Baseline\n",
      "================================================================================\n",
      "Best validation R¬≤: 0.7179 (time alone: 0.7179, baseline: 0.7139)\n",
      "Best weight on time model: 1.00\n",
      "MAPE: 32.62% (time: 32.62%, baseline: 32.76%)\n",
      "MAE:  ‚Ç¨49.02 (time: ‚Ç¨49.02, baseline: ‚Ç¨49.55)\n",
      "\n",
      "Blend did not beat the time model; keep the time submission.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINE BLEND: Time + Baseline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'y_val_pred_time' not in locals() or 'test_predictions_time' not in locals():\n",
    "    print(\"Time model predictions not available; run the time strategy cell first.\")\n",
    "elif 'y_val_pred_baseline' not in locals():\n",
    "    print(\"Baseline predictions not available; run baseline cell first.\")\n",
    "else:\n",
    "    weights = np.linspace(0, 1, 21)  # 0.0 to 1.0 step 0.05\n",
    "    best = {\n",
    "        'r2': time_r2,\n",
    "        'mape': time_mape,\n",
    "        'mae': time_mae,\n",
    "        'w': 1.0  # weight on time model\n",
    "    }\n",
    "    \n",
    "    for w in weights:\n",
    "        blend_val = w * y_val_pred_time + (1 - w) * y_val_pred_baseline\n",
    "        r2 = r2_score(y_val, blend_val)\n",
    "        mape = calculate_mape(y_val, blend_val)\n",
    "        mae = mean_absolute_error(y_val, blend_val)\n",
    "        if r2 > best['r2']:\n",
    "            best = {\n",
    "                'r2': r2,\n",
    "                'mape': mape,\n",
    "                'mae': mae,\n",
    "                'w': w\n",
    "            }\n",
    "    \n",
    "    print(f\"Best validation R¬≤: {best['r2']:.4f} (time alone: {time_r2:.4f}, baseline: {val_r2_baseline:.4f})\")\n",
    "    print(f\"Best weight on time model: {best['w']:.2f}\")\n",
    "    print(f\"MAPE: {best['mape']:.2f}% (time: {time_mape:.2f}%, baseline: {val_mape_baseline:.2f}%)\")\n",
    "    print(f\"MAE:  ‚Ç¨{best['mae']:.2f} (time: ‚Ç¨{time_mae:.2f}, baseline: ‚Ç¨{val_mae_baseline:.2f})\")\n",
    "    \n",
    "    if best['r2'] > time_r2:\n",
    "        # Blend test predictions with same weight\n",
    "        test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "        test_pred_blend = best['w'] * test_predictions_time + (1 - best['w']) * lr_baseline.predict(X_test)\n",
    "        submission_time_blend = pd.DataFrame({\n",
    "            'id': test_with_ids['id'].values,\n",
    "            'umsatz': test_pred_blend\n",
    "        })\n",
    "        submission_time_blend.to_csv('advanced_lr_predictions_time_blend.csv', index=False)\n",
    "        print(f\"\\nüéâ Improvement over time model! Saved: advanced_lr_predictions_time_blend.csv\")\n",
    "        print(f\"Submit this if validation lift is solid.\")\n",
    "    else:\n",
    "        print(\"\\nBlend did not beat the time model; keep the time submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d60f22",
   "metadata": {},
   "source": [
    "---\n",
    "## 20. Time Model Tweaks: Winsor Relax + Group Bias\n",
    "\n",
    "Two quick tweaks on top of the time model:\n",
    "- Relax winsorization upper tail (99.7%)\n",
    "- Per-Warengruppe residual bias correction\n",
    "\n",
    "Run in order; each saves a CSV only if validation improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8af1f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIME MODEL TWEAK 1: Relax Winsor Upper Tail (99.7%)\n",
      "================================================================================\n",
      "Validation R¬≤: 0.7169 (time model: 0.7179)\n",
      "MAPE: 32.74% (time model: 32.62%)\n",
      "MAE:  ‚Ç¨49.16 (time model: ‚Ç¨49.02)\n",
      "\n",
      "No gain vs time model; keep existing best.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TIME MODEL TWEAK 1: Relax Winsor Upper Tail (99.7%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rebuild time features with slightly relaxed clipping\n",
    "def build_time_data(upper_clip=99.7):\n",
    "    t_train = train_data.copy()\n",
    "    t_val = val_data.copy()\n",
    "    t_test = test_data.copy()\n",
    "    \n",
    "    # time feats\n",
    "    pi = np.pi\n",
    "    for df in [t_train, t_val, t_test]:\n",
    "        df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "        df['week_sin'] = np.sin(2 * pi * df['weekofyear'] / 52)\n",
    "        df['week_cos'] = np.cos(2 * pi * df['weekofyear'] / 52)\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # school holiday flag if available\n",
    "    school_cols = [c for c in ['SchoolHoliday', 'School_Holiday', 'school_holiday'] if c in t_train.columns]\n",
    "    if school_cols:\n",
    "        school_col = school_cols[0]\n",
    "        def to_flag(series):\n",
    "            if series.dtype in ['int64', 'float64']:\n",
    "                return series.fillna(0).astype(int).clip(0, 1)\n",
    "            return series.fillna('0').apply(lambda x: 1 if str(x) in ['1', 'True', 'true'] else 0)\n",
    "        for df in [t_train, t_val, t_test]:\n",
    "            df['school_holiday_flag'] = to_flag(df[school_col])\n",
    "    elif 'Is_Holiday' in t_train.columns:\n",
    "        for df in [t_train, t_val, t_test]:\n",
    "            df['school_holiday_flag'] = df['Is_Holiday'].apply(lambda x: 1 if x in [1, True, '1'] else 0)\n",
    "    \n",
    "    # Warengruppe mean target encoding\n",
    "    if 'Warengruppe' in t_train.columns:\n",
    "        wg_mean = t_train.groupby('Warengruppe')['umsatz'].mean()\n",
    "        global_mean = t_train['umsatz'].mean()\n",
    "        for df in [t_train, t_val, t_test]:\n",
    "            df['warengruppe_mean_sales'] = df['Warengruppe'].map(wg_mean).fillna(global_mean)\n",
    "    else:\n",
    "        global_mean = t_train['umsatz'].mean()\n",
    "        for df in [t_train, t_val, t_test]:\n",
    "            df['warengruppe_mean_sales'] = global_mean\n",
    "    \n",
    "    extra_numeric = ['week_sin', 'week_cos', 'quarter', 'warengruppe_mean_sales']\n",
    "    if 'school_holiday_flag' in t_train.columns:\n",
    "        extra_numeric.append('school_holiday_flag')\n",
    "    \n",
    "    time_numeric = [c for c in core_numeric + extra_numeric if c in t_train.columns]\n",
    "    time_categorical = [c for c in core_categorical if c in t_train.columns]\n",
    "    core_cols_time = time_numeric + time_categorical + ['umsatz', 'date']\n",
    "    t_train_sub = t_train[[c for c in core_cols_time if c in t_train.columns]].copy()\n",
    "    t_val_sub = t_val[[c for c in core_cols_time if c in t_val.columns]].copy()\n",
    "    t_test_sub = t_test[[c for c in core_cols_time if c in t_test.columns]].copy()\n",
    "    \n",
    "    # encoding\n",
    "    exclude_cols = ['date', 'umsatz']\n",
    "    cat_cols_time = [c for c in time_categorical if c in t_train_sub.columns]\n",
    "    all_time = pd.concat([t_train_sub, t_val_sub, t_test_sub], ignore_index=True)\n",
    "    all_time_enc = pd.get_dummies(all_time, columns=cat_cols_time, drop_first=True, dtype=int)\n",
    "    \n",
    "    t_train_enc = all_time_enc.iloc[:len(t_train_sub)].copy()\n",
    "    t_val_enc = all_time_enc.iloc[len(t_train_sub):len(t_train_sub)+len(t_val_sub)].copy()\n",
    "    t_test_enc = all_time_enc.iloc[len(t_train_sub)+len(t_val_sub):].copy()\n",
    "    \n",
    "    feature_cols_time = [col for col in t_train_enc.columns if col not in exclude_cols]\n",
    "    X_tr = t_train_enc[feature_cols_time].fillna(0)\n",
    "    X_va = t_val_enc[feature_cols_time].fillna(0)\n",
    "    X_te = t_test_enc[feature_cols_time].fillna(0)\n",
    "    y_tr = t_train_enc['umsatz']\n",
    "    y_va = t_val_enc['umsatz']\n",
    "    \n",
    "    # winsor target\n",
    "    p_low, p_high = np.percentile(y_tr, [0.5, upper_clip])\n",
    "    y_tr_clip = y_tr.clip(p_low, p_high)\n",
    "    \n",
    "    return X_tr, y_tr_clip, X_va, y_va, X_te\n",
    "\n",
    "# Build with relaxed upper clip\n",
    "X_tr_r, y_tr_r, X_va_r, y_va_r, X_te_r = build_time_data(upper_clip=99.7)\n",
    "\n",
    "lr_time_relaxed = LinearRegression()\n",
    "lr_time_relaxed.fit(X_tr_r, y_tr_r)\n",
    "\n",
    "y_va_pred_r = lr_time_relaxed.predict(X_va_r)\n",
    "\n",
    "time_r_mape = calculate_mape(y_va_r, y_va_pred_r)\n",
    "time_r_r2 = r2_score(y_va_r, y_va_pred_r)\n",
    "time_r_mae = mean_absolute_error(y_va_r, y_va_pred_r)\n",
    "\n",
    "print(f\"Validation R¬≤: {time_r_r2:.4f} (time model: {time_r2:.4f})\")\n",
    "print(f\"MAPE: {time_r_mape:.2f}% (time model: {time_mape:.2f}%)\")\n",
    "print(f\"MAE:  ‚Ç¨{time_r_mae:.2f} (time model: ‚Ç¨{time_mae:.2f})\")\n",
    "\n",
    "if time_r_r2 > time_r2:\n",
    "    test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "    test_pred_r = lr_time_relaxed.predict(X_te_r)\n",
    "    submission_time_relaxed = pd.DataFrame({\n",
    "        'id': test_with_ids['id'].values,\n",
    "        'umsatz': test_pred_r\n",
    "    })\n",
    "    submission_time_relaxed.to_csv('advanced_lr_predictions_time_relaxed.csv', index=False)\n",
    "    print(\"\\nüéâ Improvement! Saved: advanced_lr_predictions_time_relaxed.csv\")\n",
    "else:\n",
    "    print(\"\\nNo gain vs time model; keep existing best.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a7ff1",
   "metadata": {},
   "source": [
    "---\n",
    "## 21. Time Model Bias Correction per Warengruppe\n",
    "\n",
    "Adjust time-model predictions by adding back mean residual per Warengruppe (computed on validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe99becc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIME MODEL TWEAK 2: Per-Warengruppe Bias Correction\n",
      "================================================================================\n",
      "Validation R¬≤ (bias-corrected): 0.7233 vs time model 0.7179\n",
      "MAPE: 31.36% vs 32.62%\n",
      "MAE:  ‚Ç¨48.18 vs ‚Ç¨49.02\n",
      "\n",
      "üéâ Improvement! Saved: advanced_lr_predictions_time_bias.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TIME MODEL TWEAK 2: Per-Warengruppe Bias Correction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'y_val_pred_time' not in locals() or 'y_val_time' not in locals() or 'val_time' not in locals():\n",
    "    print(\"Time model outputs not available; run the time model cell first.\")\n",
    "elif 'Warengruppe' not in val_time.columns:\n",
    "    print(\"Warengruppe not in validation data; skipping bias correction.\")\n",
    "else:\n",
    "    # Compute residuals on validation\n",
    "    val_residuals = y_val_time - y_val_pred_time\n",
    "    grp_bias = val_time.groupby('Warengruppe').apply(lambda df: val_residuals[df.index].mean())\n",
    "    global_bias = val_residuals.mean()\n",
    "    \n",
    "    # Apply bias to validation preds\n",
    "    bias_series_val = val_time['Warengruppe'].map(grp_bias).fillna(global_bias)\n",
    "    y_val_pred_bias = y_val_pred_time + bias_series_val.values\n",
    "    \n",
    "    r2_bias = r2_score(y_val_time, y_val_pred_bias)\n",
    "    mape_bias = calculate_mape(y_val_time, y_val_pred_bias)\n",
    "    mae_bias = mean_absolute_error(y_val_time, y_val_pred_bias)\n",
    "    \n",
    "    print(f\"Validation R¬≤ (bias-corrected): {r2_bias:.4f} vs time model {time_r2:.4f}\")\n",
    "    print(f\"MAPE: {mape_bias:.2f}% vs {time_mape:.2f}%\")\n",
    "    print(f\"MAE:  ‚Ç¨{mae_bias:.2f} vs ‚Ç¨{time_mae:.2f}\")\n",
    "    \n",
    "    if r2_bias > time_r2:\n",
    "        # Apply to test preds\n",
    "        if 'test_predictions_time' in locals():\n",
    "            test_with_ids = pd.read_csv('/workspaces/Room_7_Bakery_Prediction/0_DataPreparation/0.6 Merge with test dataset and split/test_data.csv')\n",
    "            bias_series_test = test_time['Warengruppe'].map(grp_bias).fillna(global_bias)\n",
    "            test_pred_bias = test_predictions_time + bias_series_test.values\n",
    "            submission_time_bias = pd.DataFrame({\n",
    "                'id': test_with_ids['id'].values,\n",
    "                'umsatz': test_pred_bias\n",
    "            })\n",
    "            submission_time_bias.to_csv('advanced_lr_predictions_time_bias.csv', index=False)\n",
    "            print(\"\\nüéâ Improvement! Saved: advanced_lr_predictions_time_bias.csv\")\n",
    "        else:\n",
    "            print(\"Test predictions from time model not found; rerun time model cell first.\")\n",
    "    else:\n",
    "        print(\"\\nBias correction did not beat the time model; keep existing best.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
